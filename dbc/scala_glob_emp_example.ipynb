{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aab58c4-d9e8-47d6-9c6e-d93e70a7a6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.File\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.io.Source\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.nio.file.Paths\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.File\n",
    "import scala.io.Source\n",
    "import java.nio.file.Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5a4947b-fa78-4ab0-8e0b-371035687e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/07/25 08:21:24 WARN Utils: Your hostname, CALEB34NB resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/07/25 08:21:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/07/25 08:21:24 INFO SparkContext: Running Spark version 3.4.1\n",
      "25/07/25 08:21:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/25 08:21:24 INFO ResourceUtils: ==============================================================\n",
      "25/07/25 08:21:24 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/07/25 08:21:24 INFO ResourceUtils: ==============================================================\n",
      "25/07/25 08:21:24 INFO SparkContext: Submitted application: Spark Notebook\n",
      "25/07/25 08:21:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/07/25 08:21:24 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/07/25 08:21:24 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/07/25 08:21:24 INFO SecurityManager: Changing view acls to: jrob\n",
      "25/07/25 08:21:24 INFO SecurityManager: Changing modify acls to: jrob\n",
      "25/07/25 08:21:24 INFO SecurityManager: Changing view acls groups to: \n",
      "25/07/25 08:21:24 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/07/25 08:21:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: jrob; groups with view permissions: EMPTY; users with modify permissions: jrob; groups with modify permissions: EMPTY\n",
      "25/07/25 08:21:24 INFO Utils: Successfully started service 'sparkDriver' on port 46003.\n",
      "25/07/25 08:21:24 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/07/25 08:21:24 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/07/25 08:21:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/07/25 08:21:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/07/25 08:21:24 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/07/25 08:21:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b805e6a9-f6a8-4674-a3ff-446aa7abe524\n",
      "25/07/25 08:21:25 INFO MemoryStore: MemoryStore started with capacity 2.1 GiB\n",
      "25/07/25 08:21:25 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/07/25 08:21:25 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/07/25 08:21:25 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/25 08:21:25 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "25/07/25 08:21:25 INFO Executor: Starting executor ID driver on host 10.255.255.254\n",
      "25/07/25 08:21:25 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/07/25 08:21:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34203.\n",
      "25/07/25 08:21:25 INFO NettyBlockTransferService: Server created on 10.255.255.254:34203\n",
      "25/07/25 08:21:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/07/25 08:21:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.255.255.254, 34203, None)\n",
      "25/07/25 08:21:25 INFO BlockManagerMasterEndpoint: Registering block manager 10.255.255.254:34203 with 2.1 GiB RAM, BlockManagerId(driver, 10.255.255.254, 34203, None)\n",
      "25/07/25 08:21:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.255.255.254, 34203, None)\n",
      "25/07/25 08:21:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.255.255.254, 34203, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.col\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@7f233fd7\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.4.1`\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val spark = SparkSession.builder()\n",
    "  .appName(\"Spark Notebook\")\n",
    "  .master(\"local[*]\")\n",
    "  .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50765cd4-712b-40bc-ae4c-b8e611116f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jrob/proj_de_scala_training/input\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mv_path\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/jrob/proj_de_scala_training/input\"\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val v_path = Paths.get(\"\").toAbsolutePath.normalize.toString().replace(\"notebooks\",\"\") + \"input\"\n",
    "println(v_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dba60e64-a5f4-413a-a41d-a844e39647cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jrob/proj_de_scala_training/input/jobs__1__1.csv\n",
      "+------+-------------------+\n",
      "|job_id|          job_title|\n",
      "+------+-------------------+\n",
      "|     1|Marketing Assistant|\n",
      "|     2|           VP Sales|\n",
      "|     3| Biostatistician IV|\n",
      "+------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mv_input_jobs\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/jrob/proj_de_scala_training/input/jobs__1__1.csv\"\u001b[39m\n",
       "\u001b[36mdf_jobs\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [job_id: int, job_title: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val v_input_jobs = v_path + \"/\" + \"jobs__1__1.csv\"\n",
    "println(v_input_jobs)\n",
    "\n",
    "val df_jobs = (spark.read\n",
    "               .option(\"header\",\"false\")\n",
    "               .option(\"inferSchema\",\"true\")\n",
    "               .csv(v_input_jobs).toDF(\"job_id\",\"job_title\")\n",
    ")\n",
    "df_jobs.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5945df6-1165-45f0-afbe-59ba1916500c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jrob/proj_de_scala_training/input/departments__1___1_.csv\n",
      "+-------+--------------------+\n",
      "|dept_id|           dept_name|\n",
      "+-------+--------------------+\n",
      "|      1|  Product Management|\n",
      "|      2|               Sales|\n",
      "|      3|Research and Deve...|\n",
      "+-------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mv_input_deps\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/jrob/proj_de_scala_training/input/departments__1___1_.csv\"\u001b[39m\n",
       "\u001b[36mdf_deps\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [dept_id: int, dept_name: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val v_input_deps = v_path + \"/\" + \"departments__1___1_.csv\"\n",
    "println(v_input_deps)\n",
    "\n",
    "val df_deps = (spark.read\n",
    "               .option(\"header\",\"false\")\n",
    "               .option(\"inferSchema\",\"true\")\n",
    "               .csv(v_input_deps).toDF(\"dept_id\",\"dept_name\")\n",
    ")\n",
    "df_deps.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c808d49-c976-4ffb-becc-8799a70e49b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jrob/proj_de_scala_training/input/jobs__1__1.csv\n",
      "+------+-----------+-------------------+-------+------+\n",
      "|emp_id|   emp_name|          hire_date|dept_id|job_id|\n",
      "+------+-----------+-------------------+-------+------+\n",
      "|     1|Harold Vogt|2021-11-06 22:48:42|      2|    96|\n",
      "|     2|   Ty Hofer|2021-05-30 01:43:46|      8|  null|\n",
      "|     3|Lyman Hadye|2021-09-01 19:27:38|      5|    52|\n",
      "+------+-----------+-------------------+-------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mv_input_emps\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/jrob/proj_de_scala_training/input/hired_employees__1___1_.csv\"\u001b[39m\n",
       "\u001b[36mdf_emps\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [emp_id: int, emp_name: string ... 3 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val v_input_emps = v_path + \"/\" + \"hired_employees__1___1_.csv\"\n",
    "println(v_input_jobs)\n",
    "\n",
    "val df_emps = (spark.read\n",
    "               .option(\"header\",\"false\")\n",
    "               .option(\"inferSchema\",\"true\")\n",
    "               .csv(v_input_emps).toDF(\"emp_id\",\"emp_name\",\"hire_date\",\"dept_id\",\"job_id\")\n",
    ")\n",
    "df_emps.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6e1f311-4f72-4cf3-8108-6419f7441bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+--------------+-------------------+--------------------+---------------+\n",
      "|dept_id|job_id|emp_id|      emp_name|          hire_date|           job_title|      dept_name|\n",
      "+-------+------+------+--------------+-------------------+--------------------+---------------+\n",
      "|      2|    96|     1|   Harold Vogt|2021-11-06 22:48:42|    Health Coach III|          Sales|\n",
      "|      8|  null|     2|      Ty Hofer|2021-05-30 01:43:46|                null|        Support|\n",
      "|      5|    52|     3|   Lyman Hadye|2021-09-01 19:27:38|Structural Analys...|    Engineering|\n",
      "|     12|    71|     4| Lotti Crowthe|2021-10-01 09:04:21|     Statistician II|     Accounting|\n",
      "|      6|    80|     5|Gretna Lording|2021-10-10 18:22:17|    Quality Engineer|Human Resources|\n",
      "+-------+------+------+--------------+-------------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.col\u001b[39m\n",
       "\u001b[36mdf_factEmps\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [dept_id: int, job_id: int ... 5 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val df_factEmps = df_emps\n",
    "  .join(df_jobs, Seq(\"job_id\"), \"left\")\n",
    "  .join(df_deps, Seq(\"dept_id\"), \"left\")\n",
    "\n",
    "df_factEmps.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dd84dd3-66ad-4543-936d-6bea8f8ba659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+--------------+-------------------+--------------------+---------------+\n",
      "|dept_id|job_id|emp_id|      emp_name|          hire_date|           job_title|      dept_name|\n",
      "+-------+------+------+--------------+-------------------+--------------------+---------------+\n",
      "|      2|    96|     1|   Harold Vogt|2021-11-06 22:48:42|    Health Coach III|          Sales|\n",
      "|      8|     0|     2|      Ty Hofer|2021-05-30 01:43:46|           Not Found|        Support|\n",
      "|      5|    52|     3|   Lyman Hadye|2021-09-01 19:27:38|Structural Analys...|    Engineering|\n",
      "|     12|    71|     4| Lotti Crowthe|2021-10-01 09:04:21|     Statistician II|     Accounting|\n",
      "|      6|    80|     5|Gretna Lording|2021-10-10 18:22:17|    Quality Engineer|Human Resources|\n",
      "+-------+------+------+--------------+-------------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_factEmps_cleaned\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [dept_id: int, job_id: int ... 5 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// add a 0 value for the null cases\n",
    "val df_factEmps_cleaned = df_factEmps.na.fill(Map(\n",
    "  \"job_id\" -> 0,\n",
    "  \"dept_id\" -> 0,\n",
    "  \"hire_date\" -> \"1900-01-01\",  \n",
    "  \"job_title\" -> \"Not Found\",\n",
    "  \"dept_name\" -> \"Not Found\",\n",
    "))\n",
    "\n",
    "df_factEmps_cleaned.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51b6459b-1039-48dc-82de-a2bda685fe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+--------------+-------------------+--------------------+---------------+----------+-------+-------+-------+-----------+\n",
      "|dept_id|job_id|emp_id|      emp_name|          hire_date|           job_title|      dept_name|   hire_dd|hire_qq|hire_yy|hire_mm|hire_period|\n",
      "+-------+------+------+--------------+-------------------+--------------------+---------------+----------+-------+-------+-------+-----------+\n",
      "|      2|    96|     1|   Harold Vogt|2021-11-06 22:48:42|    Health Coach III|          Sales|2021-11-06|      4|   2021|     11|     202111|\n",
      "|      8|     0|     2|      Ty Hofer|2021-05-30 01:43:46|           Not Found|        Support|2021-05-30|      2|   2021|      5|     202105|\n",
      "|      5|    52|     3|   Lyman Hadye|2021-09-01 19:27:38|Structural Analys...|    Engineering|2021-09-01|      3|   2021|      9|     202109|\n",
      "|     12|    71|     4| Lotti Crowthe|2021-10-01 09:04:21|     Statistician II|     Accounting|2021-10-01|      4|   2021|     10|     202110|\n",
      "|      6|    80|     5|Gretna Lording|2021-10-10 18:22:17|    Quality Engineer|Human Resources|2021-10-10|      4|   2021|     10|     202110|\n",
      "+-------+------+------+--------------+-------------------+--------------------+---------------+----------+-------+-------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m\n",
       "\u001b[36mdf_factEmps_wrangled\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [dept_id: int, job_id: int ... 10 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val df_factEmps_wrangled = df_factEmps_cleaned\n",
    "  .withColumn(\"hire_dd\", to_date(col(\"hire_date\"), \"yyyy-MM-dd\"))\n",
    "  .withColumn(\"hire_qq\", quarter(col(\"hire_dd\")))\n",
    "  .withColumn(\"hire_yy\", year(col(\"hire_dd\")))\n",
    "  .withColumn(\"hire_mm\", month(col(\"hire_dd\")))\n",
    "  .withColumn(\"hire_period\", year(col(\"hire_dd\"))*100 + month(col(\"hire_dd\")))\n",
    "\n",
    "df_factEmps_wrangled.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08b49e68-eb84-4865-a9ff-f320f5225f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---+---+---+---+-----------+\n",
      "|           dept_name|hire_yy|  1|  2|  3|  4|total_hired|\n",
      "+--------------------+-------+---+---+---+---+-----------+\n",
      "|             Support|   2021| 34| 66| 59| 62|        221|\n",
      "|         Engineering|   2021| 27| 47| 61| 73|        208|\n",
      "|     Human Resources|   2021| 29| 56| 48| 72|        205|\n",
      "|            Services|   2021| 30| 67| 57| 51|        205|\n",
      "|Business Development|   2021| 27| 58| 49| 54|        188|\n",
      "+--------------------+-------+---+---+---+---+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m\n",
       "\u001b[36mdf_factEmps_Qtr_Depts\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [dept_name: string, hire_yy: int ... 5 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val df_factEmps_Qtr_Depts = df_factEmps_wrangled\n",
    "  .groupBy(col(\"dept_name\"), col(\"hire_yy\"))\n",
    "  .pivot(\"hire_qq\", Seq(1, 2, 3, 4))\n",
    "  .agg(count(\"emp_id\"))\n",
    "  .withColumn(\"total_hired\", coalesce(col(\"1\"), lit(0)) + coalesce(col(\"2\"), lit(0)) + coalesce(col(\"3\"), lit(0)) + coalesce(col(\"4\"), lit(0)))\n",
    "  .orderBy(col(\"total_hired\").desc, col(\"dept_name\"), col(\"hire_yy\"))\n",
    "\n",
    "df_factEmps_Qtr_Depts.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25fef3cc-1164-4252-9be9-155f5b65b4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+\n",
      "|dept_id|dept_name  |total_hired|\n",
      "+-------+-----------+-----------+\n",
      "|8      |Support    |221        |\n",
      "|5      |Engineering|208        |\n",
      "|7      |Services   |205        |\n",
      "+-------+-----------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m\n",
       "\u001b[36mdf_2021\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [dept_id: int, job_id: int ... 10 more fields]\n",
       "\u001b[36mdf_dept_hires\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [dept_id: int, dept_name: string ... 1 more field]\n",
       "\u001b[36mmean_hires\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m129.92307692307693\u001b[39m\n",
       "\u001b[36mdf_above_avg\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [dept_id: int, dept_name: string ... 1 more field]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// Step 1: Filter for 2021\n",
    "val df_2021 = df_factEmps_wrangled.filter(col(\"hire_yy\") === 2021)\n",
    "\n",
    "// Step 2: Count number of employees hired per department\n",
    "val df_dept_hires = df_2021\n",
    "  .groupBy(col(\"dept_id\"), col(\"dept_name\"))\n",
    "  .agg(count(\"emp_id\").alias(\"total_hired\"))\n",
    "\n",
    "// Step 3: Calculate the average hires across all departments\n",
    "val mean_hires = df_dept_hires\n",
    "  .agg(avg(\"total_hired\"))\n",
    "  .first()\n",
    "  .getDouble(0) // this extracts the mean as a Double\n",
    "\n",
    "// Step 4: Filter departments above the mean\n",
    "val df_above_avg = df_dept_hires\n",
    "  .filter(col(\"total_hired\") > mean_hires)\n",
    "  .orderBy(col(\"total_hired\").desc)\n",
    "\n",
    "df_above_avg.show(3,truncate = false)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
